{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPH1t2efKxZTOjo4FcfJn3P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Fundamentals: Tensors and Linear Regression  \n",
        "\n",
        "This notebook provides an introduction to PyTorch fundamentals, focusing on tensor operations and implementing linear regression.\n",
        "\n",
        "It covers:\n",
        "\n",
        "* **PyTorch Tensors Essentials**: Basic tensor creation, properties (shape, dtype), and device management (CPU vs. GPU).\n",
        "* **Implementing Linear Regression (Low-Level)**: A manual implementation of linear regression using raw PyTorch tensor operations, including forward pass, MSE loss calculation, backpropagation with loss.backward(), and parameter updates using torch.no_grad().\n",
        "* **Linear Regression Using PyTorch's High-Level API**: An implementation of linear regression using torch.nn.Linear for model definition, torch.optim.SGD for optimization, and torch.nn.MSELoss for the loss function, demonstrating a more streamlined approach.  \n",
        "\n",
        "The notebook uses the California Housing dataset as an example for linear regression, showing data preparation steps like splitting, conversion to tensors, and normalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dAPv02v1XI1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based at Chapter 10 Building Neural Networks with PyTorch of \"Hands-On ML\" textbook.  \n",
        "\n",
        "Sources:  \n",
        "* [Online Chapter (subscription required)](https://learning.oreilly.com/library/view/hands-on-machine-learning/9798341607972/ch10.html)  \n",
        "* [GitHub repo with code for chapter 10](https://github.com/ageron/handson-mlp/blob/main/10_neural_nets_with_pytorch.ipynb)"
      ],
      "metadata": {
        "id": "Wgndd-Cya097"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Tensors Essentials"
      ],
      "metadata": {
        "id": "2sRX6amqGepw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr1aSSikTUUE"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can create a PyTorch tensor much like you would create a NumPy array.\n",
        "X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5S34LaRUWq3",
        "outputId": "fc879a88-7bf6-4d3e-f012-81ab4d8032e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 4., 7.],\n",
              "        [2., 3., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a tensor’s shape and data type\n",
        "X.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "979DmZx3Um2E",
        "outputId": "a93330af-d069-4b4f-fb3d-c58a20d0b026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that tensors of strings or objects are not supported."
      ],
      "metadata": {
        "id": "B1f-xKDNFrmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtIbsiJLUqYs",
        "outputId": "7ad025ef-df3f-4994-b386-c39991a38b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means:\n",
        "* 2 → the number of rows (samples, observations, or examples)  \n",
        "* 3 → the number of columns (features, input dimensions, or variables)\n",
        "\n",
        "You can think of this as:\n",
        "> “We have 2 samples, each with 3 features.”\n",
        "\n",
        "Or in the context of machine learning:\n",
        "\n",
        "> Each row = one training example.  \n",
        "> Each column = one feature describing that example."
      ],
      "metadata": {
        "id": "U5aXaP4TMPcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the default precision for floats is 32-bits in PyTorch, whereas it’s 64-bits in NumPy. It’s generally better to use 32-bits in deep learning because this takes half the RAM and speeds up computations, and neural nets do not actually need the extra precision offered by 64-bit floats. So when calling the torch.tensor() function to convert a NumPy array to a tensor, it’s best to specify dtype=torch.float32. Alternatively, you can use torch.FloatTensor() which automatically converts the array to 32-bits:"
      ],
      "metadata": {
        "id": "f4VqaEdNVJum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify device on Colab"
      ],
      "metadata": {
        "id": "zjdN_h5SGMZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\" # for Apple\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "id": "zSMqXdxQVKeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a Colab GPU Runtime, device will be equal to \"cuda\". Now let’s create a tensor on that GPU. To do that, one option is to create the tensor on the CPU, then copy it to the GPU using the to() method:"
      ],
      "metadata": {
        "id": "ECwnRy9PV5xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
        "M = M.to(device)"
      ],
      "metadata": {
        "id": "IxcGwEa2V7aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can always tell which device a tensor lives on by looking at its device attribute:"
      ],
      "metadata": {
        "id": "pCgmRqBnWHGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijAg2S9ZWEjQ",
        "outputId": "9a89a743-bc0b-49fa-e838-2b7359851a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R = M @ M.T  # run some operations on the GPU\n",
        "R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZD49yiqWVqR",
        "outputId": "46ef13b5-6461-4985-a94d-50f1f11817a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14., 32.],\n",
              "        [32., 77.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the result R also lives on the GPU. This means we can perform multiple operations on the GPU without having to transfer data back and forth between the CPU and the GPU. This is crucial in deep learning because data transfer between devices can often become a performance bottleneck."
      ],
      "metadata": {
        "id": "O2CohS_rWcik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = torch.rand((1000, 1000))  # on the CPU\n",
        "%timeit M @ M.T\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncLTEIz7WprI",
        "outputId": "7b56ccbf-6878-49ab-d791-9bbdc67d37e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.5 ms ± 464 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "M = torch.rand((1000, 1000), device=\"cuda\")  # on the GPU\n",
        "%timeit M @ M.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbwZihBSWuEf",
        "outputId": "83d02334-84a9-466c-a517-6037c58269a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "647 µs ± 12.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPUs work by breaking large operations into smaller operations and running them in parallel across thousands of cores. If the task is small, it cannot be broken up into that many pieces, and the performance gain is therefore smaller. In fact, when running many tiny tasks, it can sometimes be faster to just run the operations on the CPU."
      ],
      "metadata": {
        "id": "zENb6FTBW7ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Linear Regression"
      ],
      "metadata": {
        "id": "SHFETicQ1C7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train exercise to use California housing dataset."
      ],
      "metadata": {
        "id": "j9PCJb4JHtKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()"
      ],
      "metadata": {
        "id": "UTU0_N1E1wBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_and_valid, X_test, y_train_and_valid, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_and_valid, y_train_and_valid, random_state=42)"
      ],
      "metadata": {
        "id": "PzL4TKie2Ba0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert it to tensors and normalize it. We could use a StandardScaler for this, but let’s just use tensor operations instead, to get a bit of practice"
      ],
      "metadata": {
        "id": "piwdgiCf3LhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By default, fetch_california_housing returns NumPy arrays (float64).\n",
        "# Neural networks expect float32 tensors, so we cast with FloatTensor.\n",
        "\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_valid = torch.FloatTensor(X_valid)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "\n",
        "\n",
        "means = X_train.mean(dim=0, keepdims=True)\n",
        "stds = X_train.std(dim=0, keepdims=True)\n",
        "X_train = (X_train - means) / stds\n",
        "X_valid = (X_valid - means) / stds\n",
        "X_test = (X_test - means) / stds"
      ],
      "metadata": {
        "id": "wLM_Z2LW2S4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization brings all features roughly to a mean ≈ 0 and standard deviation ≈ 1, which:  \n",
        "* stabilizes gradient descent;  \n",
        "* helps the network converge faster;  \n",
        "* avoids one large-magnitude feature dominating the loss.  "
      ],
      "metadata": {
        "id": "5XgikMn8119i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to reshape the tensors to column vectors. (Why?)"
      ],
      "metadata": {
        "id": "3vyQ5PFf3RdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = torch.FloatTensor(y_train).reshape(-1, 1)\n",
        "y_valid = torch.FloatTensor(y_valid).reshape(-1, 1)\n",
        "y_test = torch.FloatTensor(y_test).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "xWbEABm745j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "n_features = X_train.shape[1]  # there are 8 input features"
      ],
      "metadata": {
        "id": "0tWzKSs1KMcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-y8Kmj2KOQw",
        "outputId": "f8a828ad-66a8-4d9c-c2f3-88f5af8ee659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn((n_features, 1), requires_grad=True)\n"
      ],
      "metadata": {
        "id": "Q7NztmEWKUPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.tensor(0., requires_grad=True)"
      ],
      "metadata": {
        "id": "Hid1vPMlKU-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:  \n",
        "* **requires_grad=True** tells PyTorch to track gradients during backpropagation — essential for learning via gradient descent.    \n",
        "\n",
        "Weights (w) learn how each input feature influences the prediction.  \n",
        "* a column vector with one weight per input dimension\n",
        "* Each feature (e.g., house age, number of rooms, latitude, etc.) gets its own weight.\n",
        "* These weights tell the model how strongly each feature contributes to the output.\n",
        "\n",
        "Bias (b) shifts the entire prediction up or down, regardless of inputs.\n",
        "* a single number that’s added equally to all predictions.\n",
        "\n",
        "#### Why random initialization matters?  \n",
        "* For linear regression, initializing all weights to zero would still work — because there’s only one output neuron, no symmetry problem.  \n",
        "* For neural networks, if all neurons in a layer start with identical weights, they’ll compute the same thing and receive identical gradients → they’ll remain identical forever (no diversity in learning).    \n",
        "\n",
        "✅ Random initialization breaks this “symmetry” so each neuron learns a distinct feature representation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iVB2rhfTLI4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why manual_seed can give you different results  \n",
        "\n",
        "We called torch.manual_seed() to ensure that the results are reproducible. However, PyTorch does not guarantee perfectly reproducible results across different releases, platforms, or devices, so if you do not run the code in this chapter with PyTorch 2.5 on a Colab runtime with an Nvidia T4 GPU, you may get different results.\n",
        "\n",
        "#### Deterministic vs stochastic algorithms\n",
        "\n",
        "Moreover, since a GPU splits each operation into multiple chunks and runs them in parallel, the order in which these chunks finish may vary across runs, and this may slightly affect the result due to floating point precision errors. These minor differences may compound during training, and lead to very different models. To reduce this risk, you can tell PyTorch to use only deterministic algorithms by calling torch.use_deterministic_algorithms(True). However, deterministic algorithms are often slower than stochastic ones, and some operations don’t have a deterministic version at all, so you will get an error if your code tries to use one."
      ],
      "metadata": {
        "id": "ObX9MEuVPFDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.4\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "    # forward pass\n",
        "    y_pred = X_train @ w + b\n",
        "    loss = ((y_pred - y_train) ** 2).mean() # MSE\n",
        "    # BACKPROP (AUTOGRAD)\n",
        "    loss.backward()\n",
        "    # PARAMETER UPDATE\n",
        "    with torch.no_grad(): #  PyTorch will consume less RAM and run faster since it won’t have to keep track of the computation graph.\n",
        "        b -= learning_rate * b.grad\n",
        "        w -= learning_rate * w.grad\n",
        "        # Clear accumulated gradients\n",
        "        b.grad.zero_()\n",
        "        w.grad.zero_()\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHKltg9FLN6a",
        "outputId": "0e4a6185-57f2-4ca0-eed6-263c5d7b55b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 16.158456802368164\n",
            "Epoch 2/20, Loss: 4.8793745040893555\n",
            "Epoch 3/20, Loss: 2.255225419998169\n",
            "Epoch 4/20, Loss: 1.3307636976242065\n",
            "Epoch 5/20, Loss: 0.9680693745613098\n",
            "Epoch 6/20, Loss: 0.8142675757408142\n",
            "Epoch 7/20, Loss: 0.7417045831680298\n",
            "Epoch 8/20, Loss: 0.7020700573921204\n",
            "Epoch 9/20, Loss: 0.6765917539596558\n",
            "Epoch 10/20, Loss: 0.6577963829040527\n",
            "Epoch 11/20, Loss: 0.6426151394844055\n",
            "Epoch 12/20, Loss: 0.6297222971916199\n",
            "Epoch 13/20, Loss: 0.6184941530227661\n",
            "Epoch 14/20, Loss: 0.6085968017578125\n",
            "Epoch 15/20, Loss: 0.5998216271400452\n",
            "Epoch 16/20, Loss: 0.592018723487854\n",
            "Epoch 17/20, Loss: 0.5850691795349121\n",
            "Epoch 18/20, Loss: 0.578873336315155\n",
            "Epoch 19/20, Loss: 0.573345422744751\n",
            "Epoch 20/20, Loss: 0.5684100389480591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing linear regression using PyTorch’s low-level API wasn’t too hard, but using this approach for more complex models would get really messy and difficult."
      ],
      "metadata": {
        "id": "PVJEQhZXRVab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[:3]  # pretend these are new instances\n",
        "with torch.no_grad():\n",
        "    y_pred = X_new @ w + b  # use the trained parameters to make predictions\n",
        "\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MWK-aR0fYJ5",
        "outputId": "6758cd03-4f26-4d18-9519-6e1ce6f85324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8916],\n",
              "        [1.6480],\n",
              "        [2.6577]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Using PyTorch’s High-Level API"
      ],
      "metadata": {
        "id": "dNxvFFScRWmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn  # by convention, this module is usually imported this way\n",
        "\n",
        "torch.manual_seed(42)  # to get reproducible results\n",
        "model = nn.Linear(in_features=n_features, out_features=1)"
      ],
      "metadata": {
        "id": "SXAJSZ0oRZV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most neural networks you will need to assemble many modules, as we will see later in this chapter, so you can think of modules as math LEGO® bricks."
      ],
      "metadata": {
        "id": "xm2qLLsaRuBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NrQalPKRvW8",
        "outputId": "55753ac8-b393-438b-d5b8-fb4aa04af26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.3117], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo4vzEoBR5ko",
        "outputId": "88f8a2c1-336b-46fe-ab07-deec852685c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RRm-P7sSGvQ",
        "outputId": "90cce06f-33eb-402d-e441-be8b1f4c649d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3117], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(X_train[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU3B7idtSXN0",
        "outputId": "37ab8ea7-5be0-4eb9-adb2-acde7b5bbbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4718],\n",
              "        [ 0.1131]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model, we need to create an optimizer to update the model parameters, and we must also choose a loss function:"
      ],
      "metadata": {
        "id": "hKQ5Bp5NcfAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # learning_rate variable defined above"
      ],
      "metadata": {
        "id": "GNFWuxXcc9oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer:  \n",
        "* we’re using the simple stochastic gradient descent (SGD) optimizer, which can be used for:\n",
        "  * SGD,\n",
        "  * mini-batch GD, or\n",
        "  * batch gradient descent.\n",
        "* To initialize it, we must give it the model parameters and the learning rate."
      ],
      "metadata": {
        "id": "WAvgJ8ikdEze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = nn.MSELoss()"
      ],
      "metadata": {
        "id": "QbiOrAg1dnSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function:  \n",
        "For the loss function, we create an instance of the nn.MSELoss class: this is also a module, so we can use it like a function, giving it the predictions and the targets, and it will compute the MSE"
      ],
      "metadata": {
        "id": "L97aOTiAdn2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s write a small function to train our model. we’re now using higher-level constructs rather than working directly with tensors and autograd."
      ],
      "metadata": {
        "id": "8Q-za-FSd9u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train) # nn.MSELoss() instance\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "LhrRFPAMd-n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criterion:**  \n",
        "* In PyTorch, the **loss function object** is commonly referred to as the criterion, to distinguish it from the loss value itself (which is computed at each training iteration using the criterion). In this example, it’s the MSELoss instance.\n",
        "\n",
        "The** optimizer.step()** line corresponds to the two lines that updated b and w in our earlier code.  \n",
        "\n",
        "The **optimizer.zero_grad()** line corresponds to the two lines that zeroed out b.grad and w.grad. Notice that we don’t need to use with torch.no_grad() here since this is done automatically by the optimizer, inside the step() and zero_grad() functions.  "
      ],
      "metadata": {
        "id": "qqmAuMbFeFon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMufFGANe1jk",
        "outputId": "530034dd-82cd-4ac7-9c8e-8c10a172eb7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 4.3378496170043945\n",
            "Epoch 2/20, Loss: 0.7802939414978027\n",
            "Epoch 3/20, Loss: 0.6253842115402222\n",
            "Epoch 4/20, Loss: 0.6060433983802795\n",
            "Epoch 5/20, Loss: 0.5956299304962158\n",
            "Epoch 6/20, Loss: 0.587356686592102\n",
            "Epoch 7/20, Loss: 0.5802990794181824\n",
            "Epoch 8/20, Loss: 0.5741382241249084\n",
            "Epoch 9/20, Loss: 0.5687101483345032\n",
            "Epoch 10/20, Loss: 0.5639079809188843\n",
            "Epoch 11/20, Loss: 0.5596511363983154\n",
            "Epoch 12/20, Loss: 0.5558737516403198\n",
            "Epoch 13/20, Loss: 0.5525194406509399\n",
            "Epoch 14/20, Loss: 0.5495392084121704\n",
            "Epoch 15/20, Loss: 0.5468900203704834\n",
            "Epoch 16/20, Loss: 0.5445339679718018\n",
            "Epoch 17/20, Loss: 0.5424376726150513\n",
            "Epoch 18/20, Loss: 0.5405716300010681\n",
            "Epoch 19/20, Loss: 0.5389097332954407\n",
            "Epoch 20/20, Loss: 0.5374288558959961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, you can now use it to make predictions by simply calling it like a function (preferably inside a no_grad() context):"
      ],
      "metadata": {
        "id": "-uIG0EBCe_PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[:3]\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_new)\n",
        "y_pred\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jehp4iHifF88",
        "outputId": "0672e0d0-3550-43f4-d806-3c4e651ccbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8061],\n",
              "        [1.7116],\n",
              "        [2.6973]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code explanation:  \n",
        "* The purpose of using with **torch.no_grad()** when making predictions with a PyTorch model is to **disable gradient tracking**, which can improve performance and reduce memory consumption.  \n",
        "  * The purpose of gradient tracking in machine learning models is to compute gradients across the model, which is necessary for training but not for prediction.\n",
        "  * During the training process, the model computes the gradients of the loss function with respect to its weights, which are then used to update the weights and minimize the loss. However, during the prediction phase, gradient tracking is not necessary, as the model is not learning or updating its weights."
      ],
      "metadata": {
        "id": "ixGhLITGaDzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These predictions are similar to the ones our previous model made, but not exactly the same: that’s because the nn.Linear module initializes the parameters slightly differently: it uses a uniform random distribution from -\n",
        " to\n",
        " for both the weights and the bias term"
      ],
      "metadata": {
        "id": "c2XfRZatfoxf"
      }
    }
  ]
}